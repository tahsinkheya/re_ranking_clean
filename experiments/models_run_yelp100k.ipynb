{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cornac.data import Reader\n",
    "from cornac.datasets import movielens\n",
    "from cornac.data import Dataset, FeatureModality\n",
    "from cornac.eval_methods import RatioSplit, StratifiedSplit\n",
    "from cornac.metrics import RMSE\n",
    "from cornac.models import  MF, VAECF, NeuMF, PMF, WMF\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cornac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader()\n",
    "rating_data_pd = pd.read_csv(\n",
    "    \"../data/yelp-100K/indexed_interactions.csv\",\n",
    "    sep=\"\\t\",\n",
    "    header=0,\n",
    "    names=[\"itemID\", \"Category\", \"userID\", \"Rating\", \"Gender\"],\n",
    ")\n",
    "rating_data = rating_data_pd[[\"userID\", \"itemID\", \"Rating\"]].to_numpy()\n",
    "rating_data.__len__()\n",
    "rating_data_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants = pd.read_csv(\n",
    "    \"../data/yelp-100K/i_id_mapping.csv\",\n",
    "    sep=\"\\t\",\n",
    "    header=0,\n",
    "    names=[\"item_id\", \"Category\", \"itemID\"],\n",
    ")\n",
    "restaurants = restaurants.sort_values(by=\"itemID\")\n",
    "\n",
    "unique_categories = [\n",
    "    \"Active Life & Fitness\",\n",
    "    \"Arts & Entertainment\",\n",
    "    \"Automotive\",\n",
    "    \"Bars & Nightlife\",\n",
    "    \"Coffee,Tea & Desserts\",\n",
    "    \"Drinks & Spirits\",\n",
    "    \"Education & Learning\",\n",
    "    \"Event Services\",\n",
    "    \"Family & Kids\",\n",
    "    \"Food & Restaurants\",\n",
    "    \"Health & Beauty\",\n",
    "    \"Home & Garden\",\n",
    "    \"Miscellaneous\",\n",
    "    \"Outdoor Activities\",\n",
    "    \"Public Services & Community\",\n",
    "    \"Shopping & Fashion\",\n",
    "    \"Specialty Food & Groceries\",\n",
    "    \"Sports & Recreation\",\n",
    "    \"Technology & Electronics\",\n",
    "    \"Travel & Transportation\",\n",
    "    \"Asian\",\n",
    "]\n",
    "for c in unique_categories:\n",
    "    restaurants[c] = 0\n",
    "for index, row in restaurants.iterrows():\n",
    "    cats = row[\"Category\"].split(\"|\")\n",
    "    for cat in cats:\n",
    "        restaurants.at[index, cat] = 1\n",
    "\n",
    "cat = restaurants[unique_categories]\n",
    "# cat[:1]\n",
    "item_features_numpy = cat.to_numpy()\n",
    "item_features = {\n",
    "    str(item_id): {\"category_\" + str(idx): value for idx, value in enumerate(row)}\n",
    "    for item_id, row in enumerate(item_features_numpy)\n",
    "}\n",
    "# ids = list(range(0, 3416))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv(\n",
    "    \"../data/yelp-100K/u_id_mapping.csv\",\n",
    "    sep=\"\\t\",\n",
    "    header=0,\n",
    "    names=[\"user_id\", \"Gender\", \"userID\"],\n",
    ")\n",
    "gender_map = {\"M\": 0, \"F\": 1}\n",
    "users[\"Gender\"] = users[\"Gender\"].map(gender_map)\n",
    "users = users.sort_values(by=\"userID\")\n",
    "users = users[[\"Gender\", \"userID\"]]\n",
    "\n",
    "user_features_numpy = users.to_numpy()\n",
    "print(user_features_numpy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = rating_data\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ratio_split = StratifiedSplit(\n",
    "    data=dataset,\n",
    "    test_size=0.2,\n",
    "    rating_threshold=0,\n",
    "    # val_size=0.1,\n",
    "    seed=123,\n",
    "    verbose=True,\n",
    "    fmt=\"UIR\",\n",
    ")\n",
    "\n",
    "hr_10 = cornac.metrics.HitRatio(k=20)\n",
    "ndcg_10 = cornac.metrics.NDCG(k=20)\n",
    "recall_10 = cornac.metrics.Recall(k=20)\n",
    "prec_10 = cornac.metrics.Precision(k=20)\n",
    "auc = cornac.metrics.AUC()\n",
    "map = cornac.metrics.MAP()\n",
    "\n",
    "epochs = [20, 40, 60, 80, 100]\n",
    "models = []\n",
    "# class cornac.models.vaecf.recom_vaecf.\n",
    "# VAECF(name='VAECF', k=10, autoencoder_structure=[20], act_fn='tanh',\n",
    "# likelihood='mult', n_epochs=100, batch_size=100, learning_rate=0.001, beta=1.0, trainable=True, verbose=False, seed=None, use_gpu=False)[source]\n",
    "# for i in range(len(epochs)):\n",
    "#     models.append(\n",
    "#         NeuMF(\n",
    "#            name=f\"NeuMFe={epochs[i]}\",\n",
    "#             num_factors=8,\n",
    "#             layers=( 32, 16, 8),\n",
    "#             act_fn=\"sigmoid\",\n",
    "#             reg=0.0,\n",
    "#             num_epochs=epochs[i],\n",
    "#             batch_size=256,\n",
    "#             num_neg=3,\n",
    "#             lr=0.001,\n",
    "#             learner=\"adam\",\n",
    "#             backend=\"tensorflow\",\n",
    "#             early_stopping=None,\n",
    "#             trainable=True,\n",
    "#             verbose=True,\n",
    "#             seed=123,\n",
    "#         )\n",
    "#     )\n",
    "# model = WMF(name=f'WMF{100}', k=200, lambda_u=0.01, lambda_v=0.01, a=1, b=0.01, learning_rate=0.001, batch_size=128, max_iter=100, trainable=True, verbose=True, init_params=None, seed=123)\n",
    "\n",
    "# model = MF(\n",
    "#             name=f\"MF{40}\",\n",
    "#             k=10,\n",
    "#             backend=\"cpu\",\n",
    "#             optimizer=\"adam\",\n",
    "#             max_iter=40,\n",
    "#             learning_rate=0.01,\n",
    "#             batch_size=256,\n",
    "#             lambda_reg=0.02,\n",
    "#             dropout=0.0,\n",
    "#             use_bias=True,\n",
    "#             early_stop=False,\n",
    "#             num_threads=0,\n",
    "#             trainable=True,\n",
    "#             verbose=False,\n",
    "#             init_params=None,\n",
    "#             seed=123,\n",
    "#         )\n",
    "model =  NeuMF(\n",
    "           name=f\"NeuMFe={60}\",\n",
    "            num_factors=8,\n",
    "            layers=( 32, 16, 8),\n",
    "            act_fn=\"sigmoid\",\n",
    "            reg=0.0,\n",
    "            num_epochs=60,\n",
    "            batch_size=256,\n",
    "            num_neg=3,\n",
    "            lr=0.001,\n",
    "            learner=\"adam\",\n",
    "            backend=\"tensorflow\",\n",
    "            early_stopping=None,\n",
    "            trainable=True,\n",
    "            verbose=True,\n",
    "            seed=123,\n",
    "        )\n",
    "\n",
    "# model = VAECF(\n",
    "#             k=10,\n",
    "#             autoencoder_structure=[20],\n",
    "#             name=f\"vaee{80}\",\n",
    "#             act_fn=\"tanh\",\n",
    "#             likelihood=\"mult\",\n",
    "#             n_epochs=80,\n",
    "#             batch_size=100,\n",
    "#             learning_rate=0.001,\n",
    "#             beta=1.0,\n",
    "#             seed=123,\n",
    "#             verbose=True,\n",
    "#         )\n",
    "models = [model]\n",
    "cornac.Experiment(\n",
    "    ratio_split, models=models, metrics=[hr_10, ndcg_10, recall_10, auc, prec_10, map]\n",
    ").run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = users[\"userID\"].to_numpy()\n",
    "item_ids = restaurants[\"itemID\"].to_numpy()\n",
    "item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top_k ratings for all users:\n",
    "top_k = 100\n",
    "reco_matrix = np.zeros((len(models), len(user_ids), top_k), dtype=int)\n",
    "reco_matrix_mapped_items = np.zeros(\n",
    "    (len(models), len(user_ids), len(item_ids)), dtype=int\n",
    ")\n",
    "reco_matrix_mapped_scores = np.zeros(\n",
    "    (len(models), len(user_ids), len(item_ids)), dtype=float\n",
    ")\n",
    "reco_matrix_all = np.zeros((len(models), len(user_ids), len(item_ids)), dtype=int)\n",
    "\n",
    "\n",
    "for u in user_ids:\n",
    "    for i in range(len(models)):\n",
    "        reco_items = models[i].recommend(u)\n",
    "        items_mapped, mapped_scores = models[i].rank(\n",
    "            user_idx=u, item_indices=list(item_ids)\n",
    "        )\n",
    "        reco_matrix_mapped_items[i][u] = items_mapped\n",
    "        reco_matrix_mapped_scores[i][u] = mapped_scores\n",
    "        reco_matrix_all[i][u] = reco_items\n",
    "        reco_matrix[i][u] = reco_items[:top_k]\n",
    "\n",
    "        # print(reco_matrix[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set_data = pd.DataFrame(ratio_split.test_set.uir_tuple).transpose()\n",
    "# test_set_data.columns = [\"uid\", \"iid\", \"rating\"]\n",
    "# test_set_data = test_set_data.astype({\"uid\": \"int\", \"iid\": \"int\", \"rating\": \"int\"})\n",
    "# r_global_uid_map = {v: k for k, v in ratio_split.global_uid_map.items()}\n",
    "# r_global_iid_map = {v: k for k, v in ratio_split.global_iid_map.items()}\n",
    "\n",
    "# test_set_data[\"uid\"] = test_set_data[\"uid\"].map(r_global_uid_map)\n",
    "# test_set_data[\"iid\"] = test_set_data[\"iid\"].map(r_global_iid_map)\n",
    "# test_set_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set_data = pd.DataFrame(ratio_split.train_set.uir_tuple).transpose()\n",
    "# train_set_data.columns = [\"uid\", \"iid\", \"rating\"]\n",
    "# train_set_data = train_set_data.astype({\"uid\": \"int\", \"iid\": \"int\", \"rating\": \"int\"})\n",
    "# r_global_uid_map = {v: k for k, v in ratio_split.global_uid_map.items()}\n",
    "# r_global_iid_map = {v: k for k, v in ratio_split.global_iid_map.items()}\n",
    "\n",
    "# train_set_data[\"uid\"] = train_set_data[\"uid\"].map(r_global_uid_map)\n",
    "# train_set_data[\"iid\"] = train_set_data[\"iid\"].map(r_global_iid_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"reco_matrix_neumf_yelp100k_100\", reco_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "sorted_by_values = OrderedDict(\n",
    "    sorted(models[0].iid_map.items(), key=lambda item: item[1])\n",
    ")\n",
    "keys_sorted_by_values = list(sorted_by_values.keys())\n",
    "\n",
    "reco_items_scores_all = [OrderedDict() for _ in range(len(user_ids))]\n",
    "\n",
    "for u in user_ids:\n",
    "    actual_index_u = u\n",
    "    mapped_index_u = models[0].uid_map[actual_index_u]\n",
    "    mapped_scores = reco_matrix_mapped_scores[0][mapped_index_u]\n",
    "    ordered_dict = OrderedDict(zip(keys_sorted_by_values, mapped_scores))\n",
    "    reco_items_scores_all[actual_index_u] = ordered_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"score_dicts_neumf_yelp100k.pkl\", \"wb\") as file:\n",
    "    pickle.dump(reco_items_scores_all, file)\n",
    "\n",
    "print(\"List of OrderedDicts saved to 'score_dicts.pkl'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "re-ranking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
