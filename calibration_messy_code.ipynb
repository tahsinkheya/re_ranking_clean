{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# tahsin kheya\n",
    "# last modified 07/01/2025\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# import torch\n",
    "import time\n",
    "from logging import getLogger\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import entropy\n",
    "# from mpi4py import MPI\n",
    "\n",
    "# comm = MPI.COMM_WORLD\n",
    "rank = 0\n",
    "\n",
    "\n",
    "class GreedyCalibration(object):\n",
    "    def __init__(self, config, movies, top_k, unique_genres, users, sensitive_attr, beta):\n",
    "        self.top_k = top_k\n",
    "        self.users_df = users\n",
    "        self.users_np = users.to_numpy()\n",
    "        self.beta = beta\n",
    "        self.actual_genre_dist = pd.read_csv(\n",
    "            os.path.join(config[\"user_genre_dist_file\"]),\n",
    "            sep=\"\\t\",\n",
    "        )\n",
    "        self.actual_genre_dist = self.actual_genre_dist.sort_values(by=\"userID\")\n",
    "        self.unique_genres = unique_genres\n",
    "        # self.item_df = movies\n",
    "        self.i =movies\n",
    "        self.i[unique_genres] = self.i[self.unique_genres].div(\n",
    "            self.i[self.unique_genres].sum(axis=1), axis=0\n",
    "        )\n",
    "        self.sensitive_attr = sensitive_attr\n",
    "        self.sensitive_compare_dist = []\n",
    "        self.actual_distribution_sensitive = []\n",
    "\n",
    "    def get_recom_distribution(self, reco, uid, compare_dist, alpha):\n",
    "        reco = np.array(reco)\n",
    "        # df_reco = pd.DataFrame(\n",
    "        #     {\n",
    "        #         \"userID\": np.repeat(uid, len(reco)),\n",
    "        #         \"itemID\": reco.flatten(),\n",
    "        #         \"rank\": np.tile(np.arange(1, len(reco) + 1), 1),\n",
    "        #     }\n",
    "        # )\n",
    "        selected_items = self.i[self.i['itemID'].isin(reco)]\n",
    "        selected_items = selected_items.set_index('itemID').loc[reco].reset_index()\n",
    "        selected_items = selected_items[unique_genres].to_numpy()\n",
    "       \n",
    "\n",
    "        # df_reco[\"weight_factor\"] = 1 / (df_reco[\"rank\"]) ** 0.1\n",
    "        # merged_df = pd.merge(df_reco, self.i, on=\"itemID\", how=\"inner\")\n",
    "        # for i, genre in enumerate(self.unique_genres):\n",
    "        #     merged_df[genre] = (1 - alpha) * merged_df[genre] + alpha * compare_dist[i]\n",
    "        # merged_df[self.unique_genres] = (\n",
    "        #     merged_df[\"weight_factor\"].values[:, None] * merged_df[self.unique_genres]\n",
    "        # )\n",
    "        # summed_genre = (\n",
    "        #     merged_df.groupby(\"userID\")[self.unique_genres].sum().reset_index()\n",
    "        # )\n",
    "        \n",
    "    \n",
    "        \n",
    "       \n",
    "        indices = np.arange(len(reco))\n",
    "        weights = 1 / (indices + 1) ** 0.1\n",
    "        updated_genre_proportions = (1 - alpha) * selected_items + alpha * compare_dist\n",
    "        updated_genre_proportions = weights[:, None] * updated_genre_proportions\n",
    "        updated_genre_proportions = updated_genre_proportions.sum(axis=0)\n",
    "        \n",
    "        # print(\"::\"*10)\n",
    "        # print(updated_genre_proportions)\n",
    "        # print(\"__\"*10)\n",
    "        # print(summed_genre[self.unique_genres].to_numpy())\n",
    "        # print(\"::\"*10)\n",
    "        \n",
    "       \n",
    "        \n",
    "       \n",
    "\n",
    "        return updated_genre_proportions\n",
    "\n",
    "    def compute_diversity_score(self, reco_items, uid, scores, b):\n",
    "        alpha = 0.01\n",
    "        # current_user_sensitive_attr = self.users_df.loc[\n",
    "        #     self.users_df[\"userID\"] == uid, self.sensitive_attr\n",
    "        # ].item()\n",
    "        \n",
    "        # ____________________\n",
    "        userid = self.users_df.columns.get_loc(\"userID\")\n",
    "        sens_atr = self.users_df.columns.get_loc(self.sensitive_attr)\n",
    "        filtered_rows = self.users_np[self.users_np[:, userid] == uid][0]\n",
    "        current_user_sensitive_attr= filtered_rows[sens_atr]\n",
    "        # ____________________\n",
    "        # print(f\"user sensitive attr1 {current_user_sensitive_attr} attr 2 {sens_atr}\")\n",
    "        \n",
    "        compare_dist = self.sensitive_compare_dist[current_user_sensitive_attr]\n",
    "        reco_dist = self.get_recom_distribution(reco_items, uid, compare_dist, alpha)  # sum wr(i)q˜(д|i),\n",
    "        \n",
    "        reco_dist = np.log(reco_dist)  # log sum wr(i)q˜(д|i),\n",
    "        faireness_term = np.sum(compare_dist * reco_dist)\n",
    "\n",
    "        sum_score = sum(scores[item] for item in reco_items)\n",
    "        \n",
    "        # print(f\"first method {et-st} scores {sum_score} type of recoitems {type(reco_items)}  type of score {type(scores)}\")\n",
    "        # st = time.perf_counter()\n",
    "        # sum_scores = np.sum(scores[reco_items])\n",
    "        # et = time.perf_counter()\n",
    "        # print(f\"first method {et-st} scores {sum_scores}\")\n",
    "\n",
    "        \n",
    "\n",
    "        return (sum_score, faireness_term)\n",
    "\n",
    "    def get_improved_reco(self, top_items, items, scores):\n",
    "        self.get_sensitive_genre_dist()\n",
    "\n",
    "        return self.get_new_recommendations(\n",
    "            reco=top_items, scores=scores, all_items=items\n",
    "        )\n",
    "\n",
    "    def get_sensitive_genre_dist(self):\n",
    "        actual_dist_sensitive = pd.merge(\n",
    "            self.actual_genre_dist, self.users_df, on=\"userID\"\n",
    "        )\n",
    "        sensitive_genre_weights_a = actual_dist_sensitive.groupby(self.sensitive_attr)[\n",
    "            self.unique_genres\n",
    "        ].mean()\n",
    "\n",
    "        self.actual_distribution_sensitive = sensitive_genre_weights_a.sort_index()\n",
    "\n",
    "        sensitive_compare_dist = self.actual_distribution_sensitive.sum()\n",
    "\n",
    "        self.sensitive_compare_dist = self.actual_distribution_sensitive.apply(\n",
    "            lambda r: sensitive_compare_dist - r, axis=1\n",
    "        )\n",
    "       \n",
    "        self.sensitive_compare_dist = (\n",
    "            self.sensitive_compare_dist[unique_genres].sort_index().to_numpy()\n",
    "        )\n",
    "        # print(f\"numpy array {self.sensitive_compare_dist}\")\n",
    "\n",
    "    def normalize_scores(self, diversity_scores, b):\n",
    "        # st = time.perf_counter()\n",
    "        div_scores = np.array(diversity_scores)\n",
    "        scores = div_scores[:, 0]\n",
    "        fairness = div_scores[:, 1]\n",
    "        min_score,min_fairness = np.min(scores), np.min(fairness)\n",
    "        max_score,max_fairness = np.max(scores), np.max(fairness)\n",
    "        score_norm = (scores- min_score) /(max_score-min_score)\n",
    "        fairness_norm = (fairness-min_fairness)/(max_fairness-min_fairness)\n",
    "        fair_score_norm = (1 - b) * score_norm + b * fairness_norm\n",
    "        fair_score_norm[div_scores[:, 0] == 0] = -9999\n",
    "        return fair_score_norm\n",
    "        # et = time.perf_counter()\n",
    "        \n",
    "        \n",
    "        # print(\":-:\"*20)\n",
    "        # print(f\"first method  {et-st} \")\n",
    "        # st = time.perf_counter()\n",
    "        # \n",
    "        # print(f\"beta - {b}\")\n",
    "        # min_score = min(diversity_scores, key=lambda x: x[0])[0]\n",
    "        # max_score = max(diversity_scores, key=lambda x: x[0])[0]\n",
    "# \n",
    "        # min_fairness = min(diversity_scores, key=lambda x: x[1])[1]\n",
    "        # max_fairness = max(diversity_scores, key=lambda x: x[1])[1]\n",
    "        \n",
    "        \n",
    "\n",
    "        # all_scores = []\n",
    "        # for i in range(diversity_scores.__len__()):\n",
    "            # if diversity_scores[i] != 0:\n",
    "                # score = (diversity_scores[i][0]- min_score) /(max_score-min_score)\n",
    "                # fairness_term = (diversity_scores[i][1]-min_fairness)/(max_fairness-min_fairness)\n",
    "                # print(\n",
    "                #     f\"score {score} ft {fairness_term} total {(1 - b) * score + b * fairness_term}\"\n",
    "                # )\n",
    "                # all_scores.append((1 - b) * score + b * fairness_term)\n",
    "            # else:\n",
    "                # print(f\"index {i}\")\n",
    "                # all_scores.append(-9999)\n",
    "        # et = time.perf_counter()\n",
    "        \n",
    "        # print(\"__\"*20)\n",
    "        # print(f\"second method  {et-st} \")\n",
    "\n",
    "        # print(\":-:\"*20)\n",
    "\n",
    "        return all_scores\n",
    "\n",
    "    def get_new_recommendations(self, reco, scores, all_items):\n",
    "        \"\"\"reco is 6040x50 and scores is 6040x3416\"\"\"\n",
    "        b = self.beta  # beta for the fairness term\n",
    "        # print(f\"beta {b}\")\n",
    "        all_users = []\n",
    "        top_k = self.top_k\n",
    "        num_users = len(scores)\n",
    "        # for u in range(rank*2,rank*2+2):\n",
    "        upper_bound = min(num_users, rank * 25 + 25)\n",
    "        \n",
    "        for u in range(3, num_users):\n",
    "            start_time = time.perf_counter()\n",
    "            # remaining_items = list(range(20))\n",
    "            remaining_items = reco[u]\n",
    "            # print(remaining_items)\n",
    "            u_calibrated = []\n",
    "            for k in range(top_k):\n",
    "                diversity_scores = [\n",
    "                    self.compute_diversity_score(u_calibrated + [i], u, scores[u], b)\n",
    "                    for i in remaining_items\n",
    "                ]\n",
    "                # print(f\"before {remaining_items} len {remaining_items.__len__()}\")\n",
    "                \n",
    "                norm_diversity_scores = self.normalize_scores(diversity_scores, b)\n",
    "                max_index = np.argmax(norm_diversity_scores)\n",
    "                best_item = remaining_items[max_index]\n",
    "                # print(f\"best item {best_item}\")\n",
    "                # popped_element = arr[index_to_pop     ]\n",
    "                # new_arr = np.delete(arr, index_to_pop)\n",
    "                \n",
    "                u_calibrated.append(best_item)\n",
    "                remaining_items=np.delete(remaining_items, max_index)\n",
    "                # print(f\"after {remaining_items} len {remaining_items.__len__()}\")\n",
    "                # print(u_calibrated)\n",
    "\n",
    "\n",
    "                \n",
    "                # remaining_items.pop(max_index)\n",
    "                # print(u_calibrated)\n",
    "            end_time = time.perf_counter()\n",
    "            print(f\"time taken {end_time-start_time}\")\n",
    "            print(f\"user {u} u_calibrate {u_calibrated}\")\n",
    "\n",
    "            all_users.append(u_calibrated)\n",
    "\n",
    "        return np.array(all_users)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
